{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {},
   "outputs": [],
   "source": [
    "import osmnx as ox\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import skmob\n",
    "from skmob.preprocessing import compression, detection\n",
    "from datetime import timedelta\n",
    "from shapely.geometry import Point\n",
    "from geopy.distance import geodesic "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data loading and pre-processing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace with your local path to \"timediariesanswers-0.parquet\" and \"locationeventpertime_rd\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_td = pd.read_parquet(r\"C:\\Users\\ninav\\Desktop\\Data\\SU2_contributions_parquet_rd_anonymized\\Answers\\timediariesanswers\\timediariesanswers-0.parquet\")\n",
    "df_locationeventpertime = pd.read_parquet(r\"C:\\Users\\ninav\\Desktop\\Data\\SU2_position_parquet_rd_anonymized-001\\Sensors\\locationeventpertime_rd\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading OSM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method copied from https://github.com/chiarap2/MAT_Builder/blob/cd5c73a9ba3c7bbc0297c31664916444c4fbfc06/core/modules/Enrichment.py#L98\n",
    "def download_poi_osm(list_pois, place):\n",
    "\n",
    "        # Final list of the columns that are expected to be found in the POI dataframe.\n",
    "        list_columns_df_poi = ['osmid', 'element_type', 'name', 'name:en', 'geometry', 'category']\n",
    "\n",
    "        # Here we download the POIs from OSM if the list of types of POIs is not empty.\n",
    "        gdf_ = gpd.GeoDataFrame(columns=list_columns_df_poi, crs=\"EPSG:4326\")\n",
    "        if list_pois:\n",
    "\n",
    "            print(f\"Downloading POIs from OSM for the location {place}...\")\n",
    "            for key in list_pois:\n",
    "\n",
    "                # downloading POI\n",
    "                print(f\"Downloading {key} POIs from OSM...\")\n",
    "                poi = ox.features_from_place(place, tags={key: True})\n",
    "                print(f\"Download completed!\")\n",
    "\n",
    "                # Immediately return the empty dataframe if it doesn't contain any suitable POI...\n",
    "                if poi.empty:\n",
    "                    print(f\"No POI found for category {key}!\")\n",
    "                    break\n",
    "\n",
    "                # Remove the POIs that do not have a name.\n",
    "                poi.reset_index(inplace=True)\n",
    "                poi.drop(columns='category', inplace = True, errors='ignore') # Delete the column 'category' if it exists.\n",
    "                poi.rename(columns={key: 'category'}, inplace=True)\n",
    "                poi.drop(columns = poi.columns.difference(list_columns_df_poi), inplace=True)\n",
    "                poi = poi.loc[~poi['name'].isna()]\n",
    "                poi['category'].replace({'yes': key}, inplace=True)\n",
    "\n",
    "                # And finally, concatenate this subset of POIs to the other POIs\n",
    "                # that have been added to the main dataframe so far.\n",
    "                gdf_ = pd.concat([gdf_, poi])\n",
    "\n",
    "            gdf_.reset_index(drop=True, inplace=True)\n",
    "            return gdf_\n",
    "\n",
    "list_pois = ['amenity', 'shop', 'tourism', 'aeroway', 'building', 'historic', 'healthcare', 'landuse', 'office', 'public_transport']\n",
    "poi_place = \"Trento\"\n",
    "df_poi = download_poi_osm(list_pois, poi_place)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing SU2 timediaries\n",
    "\n",
    "Load timediaries dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_td = pd.read_parquet(r\"C:\\Users\\ninav\\Desktop\\Data\\SU2_contributions_parquet_rd_anonymized\\Answers\\timediariesanswers\\timediariesanswers-0.parquet\")\n",
    "df_td['answer_datetime']  = pd.to_datetime(df_td['answertimestamp'], format='%Y%m%d%H%M%S%f')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Translate the A2 answers (where are you?) to English"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_td['answer_datetime']  = pd.to_datetime(df_td['answertimestamp'], format='%Y%m%d%H%M%S%f')\n",
    "\n",
    "rep = {\n",
    "    \"Shop, supermarket, etc\": \"Shop\",\n",
    "    \"Pizzeria, pub, bar, restaurant\": \"Restaurant\",\n",
    "    \"House (friends, others)\": \"Others home\",\n",
    "    \"Relatives Home\": \"Relatives home\",\n",
    "    \"Movie Theater, Museum, ...\": \"Cultural\",\n",
    "    \"Classroom / Laboratory\": \"Classroom\",\n",
    "    \"Classroom / Study hall\": \"Study hall\"\n",
    "}\n",
    "\n",
    "translate_it_en = {\n",
    "    'Casa, Appartamento, Stanza': 'Home, Apartment, Room',\n",
    "    'Aula / Laboratorio': 'Classroom / Laboratory',\n",
    "    'Casa (amici, altri)': 'House (friends, others)',\n",
    "    'Casa (Genitori/parenti)': 'Relatives Home',\n",
    "    'Altro luogo in universit (Corridoi cortili, ecc.)': 'Other university place',\n",
    "    'Aula / Sala studio': 'Classroom / Study hall', \n",
    "    'All aperto': 'Outdoors', \n",
    "    'Cinema, Teatro, Museo, ...': 'Movie Theater, Theater, Museum, ...',\n",
    "    'Altro luogo': 'Other place', \n",
    "    'Pizzeria, pub, bar, ristorante': 'Pizzeria, pub, bar, restaurant',\n",
    "    'Posto di lavoro': 'Work place',\n",
    "    'Mensa': 'Canteen',\n",
    "    'Negozio, supermercato, ecc': 'Shop, supermarket, etc',\n",
    "    'Palestra, struttura sportiva': 'Gym', \n",
    "    'Biblioteca UNITN': 'UNITN Library',\n",
    "    'Altra Biblioteca': 'Other Library',\n",
    "}\n",
    "\n",
    "def get_place_cat(val):\n",
    "    # Assuming translate_it_en and rep are your translation dictionaries\n",
    "    if val in translate_it_en:\n",
    "        val = translate_it_en[val]\n",
    "\n",
    "    if val in rep:\n",
    "        val = rep[val]\n",
    "\n",
    "    return val\n",
    "\n",
    "# Apply the function to create a new column 'A1_en'\n",
    "df_td['A1_en'] = df_td['A2'].apply(get_place_cat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data loading and pre-processing SU2 GPS data\n",
    "\n",
    "Swap column names to fix data inconsistency where certain columns have values swapped\n",
    "    In the original DataFrame, a data inconsistency exists where certain columns have values swapped. \n",
    "    For example 'latitude' containing longitude coordinates in some rows etc.\n",
    "        - For most users the order is correct => LAT, LNG, ALT\n",
    "        - For the users with ids [25,32,41,43,56,57,58,59,60,61,62] => LNG, ALT, LAT\n",
    "        - For users with ids [28,36] => LAT, ALT, LNG\n",
    "        - For users with ids [30,34,37,38,42] => ALT, LNG, LAT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_columns(df):\n",
    "    df = df[df['userid'] != 31]\n",
    "\n",
    "    lng_alt_lat = [25, 32, 41, 43, 56, 57, 58, 59, 60, 61, 62]\n",
    "    df.loc[df['userid'].isin(lng_alt_lat)] = df.loc[df['userid'].isin(lng_alt_lat)].rename(columns={'latitude': 'longitude', 'longitude':'altitude', 'altitude': 'latitude'})\n",
    "\n",
    "    lat_alt_lng = [28, 36,82,84,86,87,88,92,93,116,126,132]\n",
    "    df.loc[df['userid'].isin(lat_alt_lng)] = df.loc[df['userid'].isin(lat_alt_lng)].rename(columns={'longitude': 'altitude', 'altitude':'longitude'})\n",
    "\n",
    "    alt_lng_lat = [30, 34, 37, 38, 42]\n",
    "    df.loc[df['userid'].isin(alt_lng_lat)] = df.loc[df['userid'].isin(alt_lng_lat)].rename(columns={'altitude': 'latitude', 'latitude':'altitude'})\n",
    "\n",
    "    df.rename(columns={\"userid\": \"user_id\"}, inplace=True)\n",
    "\n",
    "    df['timestamp'] = pd.to_datetime(df['timestamp'], format='%Y%m%d%H%M%S%f')\n",
    "    df = df.sort_values(by=['user_id', 'timestamp'])\n",
    "\n",
    "    return df\n",
    "\n",
    "df = swap_columns(df_locationeventpertime)\n",
    "df = df[pd.to_numeric(df['accuracy'], errors='coerce') <= 20] # Filter rows with accuracy higher than 20\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add column traj_id\n",
    "We want to add a new column, \"traj_id,\" to the existing DataFrame. This column assigns a unique trajectory ID to each row based on either a time gap of more than 1 hour or a change in user_id compared to the previous row. This helps in segmenting the trajectory data into distinct trajectories based on these criteria."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_traj_id(row):\n",
    "    global current_traj_id, prev_timestamp, prev_user_id\n",
    "\n",
    "    # If it's a new user, assign a new traj_id\n",
    "    if row['user_id'] != prev_user_id:\n",
    "        current_traj_id += 1\n",
    "\n",
    "    # If the time difference is greater than 1 hour, assign a new traj_id\n",
    "    elif (row['timestamp'] - prev_timestamp) > timedelta(hours=1):\n",
    "        current_traj_id += 1\n",
    "\n",
    "    prev_timestamp = row['timestamp']\n",
    "    prev_user_id = row['user_id']\n",
    "\n",
    "    return current_traj_id\n",
    "\n",
    "# Initialize variables\n",
    "current_traj_id = 1\n",
    "prev_timestamp = df['timestamp'].iloc[0]\n",
    "prev_user_id = df['user_id'].iloc[0]\n",
    "\n",
    "# Apply the generate_traj_id function to create the 'traj_id' column\n",
    "df['traj_id'] = df.apply(generate_traj_id, axis=1)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a TrajDataFrame from the DataFrame\n",
    "tdf = skmob.TrajDataFrame(df, latitude = 'latitude', longitude = 'longitude',\n",
    "                            datetime = 'timestamp', user_id = 'user_id', trajectory_id='traj_id')\n",
    "\n",
    "print(\"Compressing the trajectories...\")\n",
    "ctdf = compression.compress(tdf, spatial_radius_km = 0.2)\n",
    "ctdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "duration = 5    # value specifying the minimum duration of a stop.\n",
    "radius =  40    # value specifying the maximum radius a stop can have.\n",
    "stdf = detection.stay_locations(tdf,\n",
    "                                stop_radius_factor = 0.5,\n",
    "                                minutes_for_a_stop = duration,\n",
    "                                spatial_radius_km = radius,\n",
    "                                leaving_time = True)\n",
    "stops = pd.DataFrame(stdf)\n",
    "\n",
    "# Here stops' index contains the IDs of the stops. We reset the index such\n",
    "# that the old index becomes a column.\n",
    "stops.reset_index(inplace=True)\n",
    "stops.rename(columns={'index': 'stop_id'}, inplace=True)\n",
    "stops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine GPS dataset (stops) and timediaries dataset\n",
    "\n",
    "Find the homes etc\n",
    "\n",
    "## Approach 1:\n",
    "We can get the timediary and then get the GPS location at the moment when the time diary was sent.\n",
    "\n",
    "## Approach 2\n",
    "We have all stop locations of a user\n",
    "We have all timediaries where the user indicated being at home.\n",
    "We want to connect these two"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect_stops_and_td(td_category):\n",
    "    prev_user = 0\n",
    "    homes_df = pd.DataFrame(columns=['lat', 'lng', 'freq'])\n",
    "    homes_users_new = {}\n",
    "\n",
    "    for index, row in stops.iterrows():\n",
    "        # STOP DATA\n",
    "        user = row['uid']\n",
    "        entry_dt = row['datetime']\n",
    "        leaving_dt = row['leaving_datetime']\n",
    "        stop_lat = row['lat']\n",
    "        stop_lng = row['lng']\n",
    "\n",
    "        if user != prev_user:\n",
    "            # for prev_user\n",
    "            if not homes_df.empty:\n",
    "                # Find the index of the row with the highest frequency\n",
    "                max_freq_index = homes_df['freq'].idxmax()\n",
    "\n",
    "                max_freq_value = homes_df.loc[max_freq_index, 'freq']\n",
    "\n",
    "                if max_freq_value > 10:\n",
    "                    # Get the lat and lng values for the row with the highest frequency\n",
    "                    max_freq_lat = homes_df.loc[max_freq_index, 'lat']\n",
    "                    max_freq_lng = homes_df.loc[max_freq_index, 'lng']\n",
    "\n",
    "                    homes_users_new[prev_user] = [max_freq_lat, max_freq_lng]\n",
    "                # else:\n",
    "                    # print(f\"Not enough timediary data points to determine home of user {prev_user}\")\n",
    "            homes_df = pd.DataFrame(columns=['lat', 'lng', 'freq'])\n",
    "\n",
    "        # find td for stop\n",
    "        td = df_td[(df_td['userid'] == user) & (df_td['answer_datetime'] >= entry_dt) & (df_td['answer_datetime'] <= leaving_dt)]\n",
    "\n",
    "        if not td.empty:\n",
    "            for td_i, td_row in td.iterrows():\n",
    "                if td_row[\"A1_en\"] == td_category:\n",
    "                    homes_existing = homes_df[(homes_df['lat'] == stop_lat) & (homes_df['lng'] == stop_lng)]\n",
    "                    \n",
    "                    if not homes_existing.empty:\n",
    "                        first_matching_row = homes_existing.iloc[0]\n",
    "\n",
    "                        # Update the 'freq' column by 1 for the first matching row\n",
    "                        homes_df.loc[first_matching_row.name, 'freq'] += 1\n",
    "                    else:\n",
    "                        # Add a new row\n",
    "                        new_row = {\"lat\": stop_lat, \"lng\": stop_lng, \"freq\": 1}\n",
    "                        homes_df.loc[len(homes_df)] = new_row \n",
    "        prev_user = user\n",
    "    \n",
    "    return homes_users_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find students home"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "homes_users_new = connect_stops_and_td(\"Home, Apartment, Room\")\n",
    "homes_users_new\n",
    "\n",
    "df_homes = pd.DataFrame(columns=['user_id', 'home_coords', 'home_lat', 'home_lng'])\n",
    "data_to_append = []\n",
    "\n",
    "for k in homes_users_new:\n",
    "    home_point = Point(homes_users_new[k][1], homes_users_new[k][0])\n",
    "    data_to_append.append({'user_id': k, 'home_coords': home_point, 'home_lat': homes_users_new[k][0], 'home_lng': homes_users_new[k][1]})\n",
    "\n",
    "# Merge the list of dictionaries into the DataFrame\n",
    "df_homes = pd.concat([df_homes, pd.DataFrame(data_to_append)], ignore_index=True)\n",
    "df_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merge stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops = stops.merge(df_homes[['user_id', 'home_coords', 'home_lat', 'home_lng']], left_on='uid', right_on='user_id', how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine stops and df_homes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add home column to stops dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Augments stops dataframe by adding a 'home' column. \n",
    "# It iterates through each stop, checks if the associated 'uid' is present in df_homes (containing home coordinates).\n",
    "# If a match is found, it calculates the distance between the stop's coordinates and the home coordinates. \n",
    "# If the distance is less than 'max_distance_home' meters, it sets the 'home' column to True; otherwise, no action is taken.\n",
    "\n",
    "max_distance_home = 10\n",
    "\n",
    "# Add a 'home' column to df_stops\n",
    "stops['home'] = None\n",
    "\n",
    "# Loop through df_stops\n",
    "for index, row in stops.iterrows():\n",
    "    uid = row['uid']\n",
    "\n",
    "    # Check if uid is in df_homes\n",
    "    if uid in df_homes['user_id'].values and uid != 131:\n",
    "        # Get 'home_coords' for the uid\n",
    "        home_coords = df_homes.loc[df_homes['user_id'] == uid, 'home_coords'].values[0]\n",
    "\n",
    "        # Create Shapely Point for the current row in df_stops\n",
    "        current_coords = Point(row['lng'], row['lat'])\n",
    "\n",
    "\n",
    "        # Compute distance\n",
    "        distance = geodesic(home_coords.coords[0], current_coords.coords[0]).meters\n",
    "\n",
    "        # Check if distance is smaller than 'max_distance_home' meters\n",
    "        if distance < max_distance_home:\n",
    "            stops.at[index, 'home'] = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_at_home = stops[stops['home'] == True].copy()\n",
    "stops_at_home"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Combine stops with POI dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Method copied from https://github.com/chiarap2/MAT_Builder/blob/cd5c73a9ba3c7bbc0297c31664916444c4fbfc06/core/modules/Enrichment.py#L399\n",
    "\n",
    "def stop_enrichment_with_pois(df_stops, df_poi, suffix, max_distance):\n",
    "    # Prepare the stops for the subsequent spatial join.\n",
    "    stops = gpd.GeoDataFrame(df_stops,\n",
    "                                geometry=gpd.points_from_xy(df_stops.lng, df_stops.lat),\n",
    "                                crs=\"EPSG:4326\")\n",
    "    stops.to_crs('epsg:3857', inplace=True)\n",
    "    stops['geometry_stop'] = stops['geometry']\n",
    "    stops['geometry'] = stops['geometry_stop'].buffer(max_distance)\n",
    "\n",
    "    \n",
    "    pois = df_poi.copy()\n",
    "    pois.to_crs('epsg:3857', inplace=True)\n",
    "\n",
    "    # Filter out the POIs without a name!\n",
    "    pois = pois.loc[pois['name'].notna(), :]\n",
    "\n",
    "    # duplicate geometry column because we loose it during the sjoin_nearest\n",
    "    pois['geometry_' + suffix] = pois['geometry']\n",
    "    pois['element_type'] = pois['element_type'].astype(str)\n",
    "    pois['osmid'] = pois['osmid'].astype(str)\n",
    "\n",
    "    # Execute the spatial left join to associate POIs to the stops.\n",
    "    enriched_stops = stops.sjoin_nearest(pois, max_distance=0.00001, how='left', rsuffix=suffix)\n",
    "  \n",
    "    # Remove the POIs that have been associated with the same stop multiple times.\n",
    "    enriched_stops.drop_duplicates(subset=['stop_id', 'osmid'], inplace=True)\n",
    "\n",
    "    # compute the distance between the stop point and the POI geometry\n",
    "    enriched_stops['distance'] = enriched_stops['geometry_stop'].distance(enriched_stops['geometry_' + suffix])\n",
    "    \n",
    "    enriched_stops = enriched_stops.sort_values(['stop_id', 'distance'])\n",
    "    enriched_stops.reset_index(drop = True, inplace = True)\n",
    "    return enriched_stops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executing stop augmentation POIs...\n"
     ]
    }
   ],
   "source": [
    "# Maximum distance from the centroid of the stops (in meters): \n",
    "max_distance = 50\n",
    "\n",
    "print(\"Executing stop augmentation POIs...\")\n",
    "\n",
    "# We filter and look only at stops that are outside the student's home because we are not interested in finding POIs close to student's home\n",
    "stops_visits = stops[~stops['stop_id'].isin(stops_at_home['stop_id'])]\n",
    "\n",
    "# Calling functions internal to this method...\n",
    "stops_enriched = stop_enrichment_with_pois(stops_visits, df_poi, 'poi', max_distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stops_enriched.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop columns\n",
    "stops_enriched = stops_enriched.dropna(subset=['osmid'])\n",
    "\n",
    "# Fix ID\n",
    "stops_enriched['uid'] = 'student_' + stops_enriched['uid'].astype(str)\n",
    "stops_enriched['osmid_uri'] = stops_enriched.apply(lambda row: f'https://www.openstreetmap.org/{row[\"element_type\"]}/{row[\"osmid\"]}', axis=1)\n",
    "stops_enriched['stop_id'] = 'stop_' + stops_enriched['stop_id'].astype(str)\n",
    "stops_enriched['tid'] = 'traj_' + stops_enriched['tid'].astype(str)\n",
    "\n",
    "stops_enriched = stops_enriched[['stop_id', 'uid', 'datetime', 'lat', 'lng', 'tid', 'leaving_datetime', 'home_lat', 'home_lng', 'osmid_uri' ]]\n",
    "stops_enriched.to_csv(\"../generated_data/stops_with_closest_poi.csv\", index=False)\n",
    "stops_enriched"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mlcourse",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
